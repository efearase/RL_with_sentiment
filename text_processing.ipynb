{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26fb5f0b",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/efearase/RL_with_sentiment/blob/main/text_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ccd18c1",
   "metadata": {
    "id": "6ccd18c1",
    "outputId": "1a73371c-ddf3-4b79-828c-671735bb923b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dostoevsky.tokenization import RegexTokenizer\n",
    "from dostoevsky.models import FastTextSocialNetworkModel\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "tokenizer = RegexTokenizer()\n",
    "tokens = tokenizer.split('всё очень плохо')\n",
    "\n",
    "model = FastTextSocialNetworkModel(tokenizer=tokenizer)\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model='blanchefort/rubert-base-cased-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca0acb3",
   "metadata": {
    "id": "dca0acb3"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/text_CBOM.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70364a1f",
   "metadata": {
    "id": "70364a1f",
    "outputId": "80c463de-92bb-4237-a2d4-bd4b57ebd845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xz/t8gqtcmx7p3f64t45x767k8m0000gn/T/ipykernel_31467/2878332882.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  s += len(df['text'][i])\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for i in range(len(df)):\n",
    "    s += len(df['text'][i])\n",
    "\n",
    "print(s//1000* 0.0020 * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8745c16-31f6-423c-8396-a51d43b50c73",
   "metadata": {
    "id": "f8745c16-31f6-423c-8396-a51d43b50c73"
   },
   "outputs": [],
   "source": [
    "def filter_text(input_text):\n",
    "    allowed_chars = set(\n",
    "        'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "        '0123456789 .,!?:;'\n",
    "        'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "        'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'\n",
    "    )\n",
    "    return ''.join(c for c in input_text if c in allowed_chars)\n",
    "\n",
    "def select_message(text, name='CBOM'):\n",
    "    arr = text.split(\".\")\n",
    "    return ''.join(sent for sent in arr if name in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747e5559-94c8-49ec-ac74-e73390734e4d",
   "metadata": {
    "id": "747e5559-94c8-49ec-ac74-e73390734e4d",
    "outputId": "a4a2ffaf-ce3b-430c-e5e6-e5c557c38ced",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "tokens = tokenizer.split('всё очень плохо')  # [('всё', None), ('очень', None), ('плохо', None)]\n",
    "\n",
    "model = FastTextSocialNetworkModel(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13eb049e-33ac-44d9-9444-52a3ab08e729",
   "metadata": {
    "id": "13eb049e-33ac-44d9-9444-52a3ab08e729"
   },
   "outputs": [],
   "source": [
    "def sentiment(df):\n",
    "    sentiment_class = []\n",
    "    sentiment_score = []\n",
    "    classifier = pipeline('sentiment-analysis', model='blanchefort/rubert-base-cased-sentiment')\n",
    "    t_0 = 0\n",
    "    print('Start fancy...')\n",
    "    batch_results = []  # Initialize batch_results to an empty list\n",
    "    for i in range(len(df)):\n",
    "        if i - t_0 > 100:\n",
    "            print(f\"progress: {round(i/len(df)*100, 2)}%\")\n",
    "            clear_output(wait=True)\n",
    "            t_0 = i\n",
    "        try:\n",
    "            res = classifier(df['text'][i])\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                # If the initial classification fails, try with a truncated version of the text\n",
    "                res = classifier(df['text'][i][:512])\n",
    "            except Exception as e:\n",
    "                # If it still fails, skip this record or use a default value\n",
    "                print(f\"Error processing text at index {i}: {e}\")\n",
    "                res = [{'label': 'neutral', 'score': 0}]  # Example default value\n",
    "        batch_results.extend(res)\n",
    "\n",
    "    sentiment_class = [res['label'] for res in batch_results]\n",
    "    sentiment_score = [res['score'] for res in batch_results]\n",
    "\n",
    "    df['sentiment_class'] = sentiment_class\n",
    "    df['sentiment_score'] = sentiment_score\n",
    "\n",
    "    dostoevski_label = []\n",
    "    dostoevski_score = []\n",
    "    \n",
    "    results = model.predict(df['text'], k=1)\n",
    "\n",
    "    for result in results:\n",
    "        dostoevski_label.append(list(result.keys())[0])\n",
    "        dostoevski_score.append(list(result.values())[0])\n",
    "\n",
    "    df['dostoevski_class'] = dostoevski_label\n",
    "    df['dostoevski_score'] = dostoevski_score\n",
    "\n",
    "    replacement_dict = {'positive': 1, 'negative': -1, 'neutral': 0,\n",
    "                       'POSITIVE': 1, 'NEGATIVE': -1, 'NEUTRAL': 0, 'speech' : 0, 'skip': 0}\n",
    "\n",
    "    df.replace(replacement_dict, inplace=True)\n",
    "\n",
    "    df['bert_score'] = df['sentiment_class']*df['sentiment_score']\n",
    "    df['dostoevski_score'] = df['dostoevski_class']*df['dostoevski_score']\n",
    "\n",
    "    df.drop(columns=['sentiment_class', 'sentiment_score', 'dostoevski_class'],\n",
    "            inplace =True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c600184-29bc-4c66-ba92-7576120478f5",
   "metadata": {
    "id": "8c600184-29bc-4c66-ba92-7576120478f5",
    "outputId": "0a98c70d-fd7c-43e9-abf5-9c3cd00cea36"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetting sentiment...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m df_new \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m---> 16\u001b[0m df_new \u001b[38;5;241m=\u001b[39m sentiment(df)\n\u001b[1;32m     17\u001b[0m df_new\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file)\n",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m, in \u001b[0;36msentiment\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     31\u001b[0m dostoevski_label \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     32\u001b[0m dostoevski_score \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 34\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     37\u001b[0m     dostoevski_label\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(result\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/dostoevsky/models.py:67\u001b[0m, in \u001b[0;36mFastTextSocialNetworkModel.predict\u001b[0;34m(self, sentences, k)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences: List[\u001b[38;5;28mstr\u001b[39m], k: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[0;32m---> 67\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_input(sentences)\n\u001b[1;32m     68\u001b[0m     Y \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(sentence, k\u001b[38;5;241m=\u001b[39mk) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m X)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m((label\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__label__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels), scores)) \u001b[38;5;28;01mfor\u001b[39;00m labels, scores \u001b[38;5;129;01min\u001b[39;00m Y]\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/dostoevsky/models.py:61\u001b[0m, in \u001b[0;36mFastTextSocialNetworkModel.preprocess_input\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(token \u001b[38;5;28;01mfor\u001b[39;00m token, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msplit(sentence, lemmatize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlemmatize))\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences\n\u001b[1;32m     64\u001b[0m     ]\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/dostoevsky/models.py:62\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(token \u001b[38;5;28;01mfor\u001b[39;00m token, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msplit(sentence, lemmatize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlemmatize))\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences\n\u001b[1;32m     64\u001b[0m     ]\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/dostoevsky/tokenization.py:30\u001b[0m, in \u001b[0;36mRegexTokenizer.split\u001b[0;34m(self, text, lemmatize)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, lemmatize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(token\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower(), \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m regex_tokenize(text)]\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/dostoevsky/tokenization.py:30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, lemmatize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(token\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower(), \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m regex_tokenize(text)]\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/razdel/substring.py:16\u001b[0m, in \u001b[0;36mfind_substrings\u001b[0;34m(chunks, text)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_substrings\u001b[39m(chunks, text):\n\u001b[1;32m     15\u001b[0m     offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[1;32m     17\u001b[0m         start \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mfind(chunk, offset)\n\u001b[1;32m     18\u001b[0m         stop \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/razdel/segmenters/tokenize.py:299\u001b[0m, in \u001b[0;36mTokenSegmenter.segment\u001b[0;34m(self, parts)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msegment\u001b[39m(\u001b[38;5;28mself\u001b[39m, parts):\n\u001b[0;32m--> 299\u001b[0m     buffer \u001b[38;5;241m=\u001b[39m safe_next(parts)\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/razdel/segmenters/base.py:9\u001b[0m, in \u001b[0;36msafe_next\u001b[0;34m(iter)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_next\u001b[39m(\u001b[38;5;28miter\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/razdel/segmenters/tokenize.py:262\u001b[0m, in \u001b[0;36mTokenSplitter.__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 262\u001b[0m     atoms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matoms(text))\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(atoms)):\n\u001b[1;32m    264\u001b[0m         atom \u001b[38;5;241m=\u001b[39m atoms[index]\n",
      "File \u001b[0;32m~/Desktop/conda/miniconda3/envs/myenv/lib/python3.11/site-packages/razdel/segmenters/tokenize.py:250\u001b[0m, in \u001b[0;36mTokenSplitter.atoms\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21matoms\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 250\u001b[0m     matches \u001b[38;5;241m=\u001b[39m ATOM\u001b[38;5;241m.\u001b[39mfinditer(text)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m matches:\n\u001b[1;32m    252\u001b[0m         start \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mstart()\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('data'):\n",
    "    print(file)\n",
    "    if file.endswith('csv') and file not in os.listdir('text_data'):\n",
    "        ticker = file.split(\"_\")[1].split(\".\")[0]\n",
    "        def select_message(text, name=ticker):\n",
    "            arr = text.split(\".\")\n",
    "            return ''.join(sent for sent in arr if name in sent)\n",
    "        df = pd.read_csv(\"data/\" + file, index_col=0)\n",
    "        try:\n",
    "            df[\"text\"].apply(filter_text)\n",
    "            df[\"text\"].apply(select_message)\n",
    "        except TypeError:\n",
    "            pass\n",
    "        print('getting sentiment...')\n",
    "        df_new = pd.DataFrame()\n",
    "        df_new = sentiment(df)\n",
    "        df_new.to_csv(\"text_data/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43ace0-2198-48e0-8d44-12db01d719a7",
   "metadata": {
    "id": "ec43ace0-2198-48e0-8d44-12db01d719a7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
